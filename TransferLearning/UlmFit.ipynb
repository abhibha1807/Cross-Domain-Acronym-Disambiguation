{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a135647335c9a49c9c477e585bfc74fe96a855a6d3ed71703ac9613ea2bf86c1",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from scipy.spatial.distance import cosine as dist"
   ]
  },
  {
   "source": [
    "## Load and Preprocess Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class prepareData:\n",
    "    def __init__(self, filename):\n",
    "        self.data=self.loadData(filename)\n",
    "        self.X=[]\n",
    "        self.Y=[]\n",
    "    \n",
    "    def loadData(self,filename):\n",
    "        data=[]\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def getLength(self):\n",
    "        return (len(self.X))\n",
    "        \n",
    "\n",
    "    def preprocessData(self):\n",
    "        #extract words in a window\n",
    "        full_forms=[]\n",
    "        for i in self.data:\n",
    "            acro_at=i['acronym']\n",
    "            tok=i['tokens']\n",
    "            full_forms.append(i['expansion'])\n",
    "            n=len(tok)\n",
    "            low=acro_at-5\n",
    "            up=acro_at+5\n",
    "            if low<0:\n",
    "                low=0\n",
    "            if up>n:\n",
    "                up=n\n",
    "            window=''\n",
    "            for j in range(low,up):\n",
    "                window=window+tok[j]+' '\n",
    "            self.X.append(window)\n",
    "\n",
    "        label_set=set(full_forms)\n",
    "        n=len(label_set)\n",
    "        l=list(label_set)\n",
    "\n",
    "        for a in full_forms:\n",
    "            for i in range(n):\n",
    "                if l[i]==a:\n",
    "                    self.Y.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=dataClass.prepareData('dataset.json')\n",
    "df = pd.DataFrame(list(zip(data.X, data.Y)),columns =['sentence', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Train language model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "# after tokenisation\n",
    "data_lm = (TextList.from_df(df, cols='sentence')\n",
    "                .split_by_rand_pct(0.1)\n",
    "                .label_for_lm()  \n",
    "                .databunch(bs=48))\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5).to_fp16()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size and learning rate\n",
    "bs=48\n",
    "lr = 1e-02\n",
    "lr *= bs/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier for one cycle\n",
    "learn.fit_one_cycle(1, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all layers and then train some more\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, lr/10, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the encoder and vocab\n",
    "learn.save('fine_tuned_10')\n",
    "learn.save_encoder('fine_tuned_enc_10')"
   ]
  },
  {
   "source": [
    "## Classification Phase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset for classification \n",
    "data_clas = (TextList.from_df(df, cols=['sentence'], vocab=data_lm.vocab)\n",
    "             .split_by_rand_pct(0.1)\n",
    "             .label_from_df(cols= 'label')\n",
    "             .databunch(bs=48))\n",
    "\n",
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising classifier\n",
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5,metrics=[accuracy]).to_fp16()\n",
    "learn_c.load_encoder('fine_tuned_enc_10')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.lr_find()\n",
    "learn_c.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.fit_one_cycle(3,lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and momentum\n",
    "learn_c.recorder.plot_losses(), learn_c.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze and train more.\n",
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(2, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  }
 ]
}